"""
T·∫£i data t·ª´ s√†n binance v·ªÅ 
T·ªïng h·ª£p th√†nh d·ªØ li·ªáu dataset
L∆∞u v√†o mongodb
L∆∞u v√†o qdrant
"""
import asyncio
import zipfile
import aiohttp
from aiohttp_socks import ProxyConnector
from datetime import datetime, timezone, timedelta
from typing import List, Optional
from colorama import Fore, Style
from pymongo import ReplaceOne
from pathlib import Path
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from qdrant_client.models import Distance, VectorParams, PointStruct
from app.models.dataset import Dataset
from app.db.init_db import db
from app.utils.log import log
from app.utils.timeframe import Timeframe, TimeframeEventValue, get_timeframe_start_end, infer_timestamp_unit
from app.utils.timeframe_timer import TimeframeTimer
from app.schemas.dataset import DatasetType 
from app.models.dataset import Dataset
from app.utils.Binance.vision import BinanceVisionData
from app.utils.types import MarketType

field_names = ["trade_id", "price", "quantity", "quote_quantity", "timestamp", "is_buyer_maker"]
ex_field_names = ["is_best_match"]


class CollectDatasetService:
    download_dir = 'import/'
    vision = BinanceVisionData()

    async def import_file_trades_csv(
        self, 
        symbol: str, 
        market_type: MarketType,
        file_csv_path: Path
    ):
        """
        Import file csv trades v·ªÅ.

        """
        # Log ƒë·ªÉ theo d√µi ƒëang ƒë·ªçc file n√†o (symbol/market/file)
        log.info(f"‚è≥ ƒë·ªçc file csv... {symbol} - {market_type} - {file_csv_path.name}")
        # n·∫øu l√† spot th√¨ kh√¥ng c√≥ header, header g·ªìm: trade_id, price, quantity, quote_quantity, timestamp, is_buyer_maker, is_best_match
        # n·∫øu l√† future th√¨ c√≥ header, header g·ªìm: trade_id, price, quantity, quote_quantity, timestamp, is_buyer_maker 
        if market_type == "spot":
            # Spot: kh√¥ng c√≥ header, b·ªè c·ªôt is_best_match
            # - header=None v√¨ file spot kh√¥ng c√≥ header
            # - names=... ƒë·ªÉ g√°n t√™n c·ªôt theo ƒë√∫ng schema m√¨nh k·ª≥ v·ªçng
            df = pd.read_csv(file_csv_path, header=None, names=[*field_names, *ex_field_names])
            # B·ªè c·ªôt is_best_match v√¨ kh√¥ng d√πng
            df = df.drop("is_best_match", axis=1)
        elif market_type == "future":
            # Future: c√≥ header, skip header row v√† ƒë·∫∑t t√™n c·ªôt m·ªõi
            # - header=0: d√≤ng ƒë·∫ßu l√† header
            # - names=field_names: chu·∫©n ho√° l·∫°i t√™n c·ªôt ƒë·ªÉ th·ªëng nh·∫•t v·ªõi nh√°nh spot
            df = pd.read_csv(file_csv_path, header=0, names=field_names)
        else:
            raise ValueError(f"Market type {market_type} kh√¥ng h·ª£p l·ªá")
        
        # Chuy·ªÉn sang x·ª≠ l√Ω d·∫°ng DataFrame: normalize d·ªØ li·ªáu, gom block, t√≠nh to√°n, v√† l∆∞u DB
        return await self.import_file_trades_pandas(symbol, market_type, df)


    async def import_file_trades_pandas(self, symbol: str, market_type: MarketType, df: pd.DataFrame):
        """
        Import file csv trades v·ªÅ.
        
        """
        # Kh√¥ng c√≥ d·ªØ li·ªáu th√¨ kh√¥ng l√†m g√¨
        if df is None or df.empty:
            return (0, 0, 0)

        # Validate: ƒë·∫£m b·∫£o c√≥ ƒë·ªß c√°c c·ªôt t·ªëi thi·ªÉu ƒë·ªÉ x·ª≠ l√Ω
        required_cols = {"price", "quantity", "timestamp", "is_buyer_maker"}
        missing = required_cols - set(df.columns)
        if missing:
            raise ValueError(f"Thi·∫øu c·ªôt b·∫Øt bu·ªôc trong dataframe: {sorted(missing)}")

        # df = df.copy()

        # Chu·∫©n ho√° ki·ªÉu d·ªØ li·ªáu (ƒë·ªçc t·ª´ CSV th∆∞·ªùng l√† string)
        df["price"] = pd.to_numeric(df["price"], errors="coerce")
        df["quantity"] = pd.to_numeric(df["quantity"], errors="coerce")
        df["timestamp"] = pd.to_numeric(df["timestamp"], errors="coerce")

        # Drop c√°c d√≤ng kh√¥ng parse ƒë∆∞·ª£c ƒë·ªÉ tr√°nh l·ªói khi t√≠nh to√°n
        df = df.dropna(subset=["price", "quantity", "timestamp", "is_buyer_maker"])
        if df.empty:
            return (0, 0, 0)

        # Sort theo th·ªùi gian (v√† trade_id n·∫øu c√≥) ƒë·ªÉ gom consecutive ch√≠nh x√°c
        if "trade_id" in df.columns:
            df["trade_id"] = pd.to_numeric(df["trade_id"], errors="coerce")
            df = df.sort_values(["timestamp", "trade_id"], kind="mergesort")
        else:
            df = df.sort_values(["timestamp"], kind="mergesort")

        # ƒê·ªãnh nghƒ©a ƒë∆°n v·ªã timestamp theo market_type
        # - spot: microseconds (us)
        # - future: milliseconds (ms)
        if market_type == "spot":
            ts_unit = "us"
        elif market_type == "future":
            ts_unit = "ms"
        else:
            raise ValueError(f"Market type {market_type} kh√¥ng h·ª£p l·ªá")

        # Chuy·ªÉn timestamp th√†nh datetime UTC
        df["time"] = pd.to_datetime(df["timestamp"].astype("int64"), unit=ts_unit, utc=True)

        # ƒê·ªìng b·ªô: chuy·ªÉn t·∫•t c·∫£ v·ªÅ microseconds ƒë·ªÉ l∆∞u th·ªëng nh·∫•t
        if ts_unit == "ms":
            # ms -> us (multiply by 1_000)
            df["timestamp_us"] = df["timestamp"] * 1_000
        else:
            # us -> us (no change)
            df["timestamp_us"] = df["timestamp"]

        # Gom c√°c giao d·ªãch li√™n ti·∫øp c√πng chi·ªÅu (is_buyer_maker) th√†nh m·ªôt "block"
        # √ù t∆∞·ªüng: m·ªói khi is_buyer_maker ƒë·ªïi gi√° tr·ªã => b·∫Øt ƒë·∫ßu block m·ªõi
        df["_block"] = df["is_buyer_maker"].ne(df["is_buyer_maker"].shift()).cumsum()
        grouped = df.groupby("_block", sort=False)

        # T·ªïng h·ª£p block:
        # - volume: sum(quantity)
        # - time: l·∫•y time cu·ªëi block (ƒë·∫°i di·ªán block)
        # - first_price, last_price: ƒë·ªÉ t√≠nh price v√† price_delta theo y√™u c·∫ßu
        # - is_buyer_maker: l·∫•y gi√° tr·ªã c·ªßa block (True/False)
        agg = grouped.agg(
            time=("time", "last"),
            first_time=("time", "first"),
            last_time=("time", "last"),
            volume=("quantity", "sum"),
            first_price=("price", "first"),
            last_price=("price", "last"),
            is_buyer_maker=("is_buyer_maker", "first"),
        ).reset_index(drop=True)

        # Gi√° ƒë·∫°i di·ªán = gi√° giao d·ªãch ƒë·∫ßu ti√™n trong block
        agg["price"] = agg["first_price"]
        # price_delta = gi√° cu·ªëi - gi√° ƒë·∫ßu
        agg["price_delta"] = agg["last_price"] - agg["first_price"]
        # is_buy = True n·∫øu is_buyer_maker == False (ng∆∞·ªùi mua l√† maker => b√°n)
        agg["is_buy"] = agg["is_buyer_maker"] == False
        agg = agg.dropna(subset=["price", "volume", "time", "first_price", "last_price", "is_buyer_maker"]) 
        if agg.empty:
            return (0, 0, 0)

        # Delta gi·ªØa c√°c block li√™n ti·∫øp (d√πng ƒë·ªÉ l√†m feature/label)
        agg["time_delta"] = agg["time"].diff().fillna(pd.Timedelta(0))

        # Map sang document Dataset ƒë·ªÉ l∆∞u MongoDB
        # L∆∞u √Ω: MongoDB kh√¥ng encode ƒë∆∞·ª£c timedelta => convert time_delta sang milliseconds (int)
        docs: List[dict] = []
        
        # Thu th·∫≠p raw features cho Qdrant
        raw_features_list = []
        for r in agg.itertuples(index=False):
            time_delta = r.time_delta
            price_delta = float(r.price_delta)
            volume = float(r.volume)
            is_buy = bool(r.is_buy)
            docs.append(
                {
                    "symbol": symbol,
                    "time": r.time.to_pydatetime() if hasattr(r.time, "to_pydatetime") else r.time,
                    "time_delta": time_delta,
                    "price": float(r.price),
                    "price_delta": price_delta,
                    "volume": volume,
                    "is_buy": is_buy,
                }
            )
            
            # Thu th·∫≠p features cho Qdrant
            is_buy_f = 1.0 if is_buy else 0.0
            raw_features_list.append([price_delta, volume, time_delta, is_buy_f])
        # Chuy·ªÉn sang numpy array
        raw_features = np.array(raw_features_list) if raw_features_list else np.empty((0, 4))

        # L∆∞u v√†o Qdrant (n·∫øu c√≥ d·ªØ li·ªáu)
        if raw_features.size > 0:
            await self.save_to_qdrant(raw_features, docs)
            
        # L∆∞u v√†o MongoDB b·∫±ng h√†m ri√™ng 
        return await self.save_to_mongodb(docs)


    async def save_to_mongodb(self, docs: List[dict]):
        """
        L∆∞u danh s√°ch documents v√†o MongoDB b·∫±ng bulk upsert.
        
        Args:
            docs: List[Dict] - danh s√°ch document Dataset ƒë·ªÉ l∆∞u
            
        Returns:
            tuple: (inserted, replaced, total)
        """
        if not docs:
            return (0, 0, 0)
            
        ops = [
            ReplaceOne(
                {"symbol": d["symbol"], "time": d["time"]},
                d,
                upsert=True,
            )
            for d in docs
        ]

        # Bulk upsert ƒë·ªÉ tƒÉng t·ªëc import (thay v√¨ insert t·ª´ng record)
        collection = Dataset.get_pymongo_collection()
        result = await collection.bulk_write(ops, ordered=False)

        inserted = len(getattr(result, "upserted_ids", {}) or {})
        replaced = int(getattr(result, "modified_count", 0) or 0)
        total = len(ops)

        return (inserted, replaced, total)


    async def save_to_qdrant(self, X_raw: np.ndarray, docs: List[dict]):
        """
        L∆∞u vectors v√† metadata v√†o Qdrant.
        
        Args:
            X_raw: numpy array - raw features [[price_delta, volume, time_delta, is_buy], ...]
            docs: List[dict] - metadata t∆∞∆°ng ·ª©ng v·ªõi m·ªói vector
        """
        if X_raw.size == 0 or not docs:
            return

        scaler = StandardScaler()
        scaler.fit(X_raw)
        joblib.dump(scaler, "qdrant_scaler.pkl")
        print("‚úÖ ƒê√£ l∆∞u scaler!")

        client = db.qdrant_client
        collection_name = "dataset"
        
        # --- 1. ƒê·∫£m b·∫£o collection t·ªìn t·∫°i ---
        try:
            client.get_collection(collection_name)
        except Exception:
            # T·∫°o m·ªõi n·∫øu ch∆∞a c√≥
            client.create_collection(
                collection_name=collection_name,
                vectors_config=VectorParams(size=4, distance=Distance.COSINE)
            )
            log.info(f"üÜï T·∫°o collection Qdrant: {collection_name}")
        
        # --- 2. Chu·∫©n h√≥a features (CH·ªà transform, kh√¥ng fit!) ---
        try:
            X_scaled = scaler.transform(X_raw)  # ‚Üê scaler ph·∫£i ƒë∆∞·ª£c fit tr∆∞·ªõc!
        except Exception as e:
            log.error(f"‚ùå L·ªói khi chu·∫©n h√≥a d·ªØ li·ªáu cho Qdrant: {e}")
            return

        # --- 3. T·∫°o points ---
        points = []
        for vector, doc in zip(X_scaled, docs):
            # T·∫°o ID ·ªïn ƒë·ªãnh: symbol + timestamp (microseconds)
            ts_us = int(doc["time"].timestamp() * 1_000_000)  # microsecond precision
            id_str = f"{doc['symbol']}_{ts_us}"
            point_id = int(hashlib.md5(id_str.encode()).hexdigest()[:16], 16)

            points.append(PointStruct(
                id=point_id,
                vector=vector.tolist(),
                payload={
                    "symbol": doc["symbol"],
                    "time": doc["time"].isoformat(),
                    "price": float(doc["price"]),
                    "price_delta": float(doc["price_delta"]),
                    "volume": float(doc["volume"]),
                    "is_buy": bool(doc["is_buy"]),
                    "time_delta_sec": float(doc["time_delta"]),  # ƒë∆°n v·ªã: gi√¢y
                }
            ))

        # --- 4. Upsert theo batch ---
        batch_size = 100
        max_retries = 3

        for i in range(0, len(points), batch_size):
            batch = points[i:i + batch_size]
            for attempt in range(max_retries):
                try:
                    client.upsert(collection_name=collection_name, points=batch)
                    log.info(f"‚úÖ ƒê√£ l∆∞u {len(batch)} vectors v√†o Qdrant (batch {i // batch_size + 1})")
                    break
                except Exception as e:
                    if attempt < max_retries - 1:
                        log.warning(f"‚ö†Ô∏è Qdrant upsert th·∫•t b·∫°i (l·∫ßn {attempt + 1}), th·ª≠ l·∫°i...: {str(e)[:100]}")
                        await asyncio.sleep(1)
                    else:
                        log.error(f"‚ùå Qdrant upsert th·∫•t b·∫°i sau {max_retries} l·∫ßn: {e}")




    async def import_month(
        self, 
        symbol: str, 
        market_type: MarketType,
        month: int, 
        year: int
    ):
        """
        T·∫£i data t·ª´ s√†n binance v·ªÅ.
        m·ªü file csv ra, ƒë·ªçc.
        chuy·ªÉn ƒë·ªïi th√†nh ƒë·ªãnh d·∫°ng dataset
        """
        data_type = 'trades'
        # t·∫£i 
        # n·∫øu file ƒë√£ t·∫£i th√¨ th√¥i 
        # zip_path = self.vision.download_trades_month(symbol, market_type, data_type, month, year, self.download_dir)
        zip_path = self.vision.download_trade(symbol, market_type, data_type, datetime(2025, 12, 1), self.download_dir)
        log.info(f"ƒê√£ t·∫£i xong file: {zip_path}, ƒëang gi·∫£i n√©n...")
        # m·ªü file 
        # ƒë·ªçc file csv
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            for name in zip_ref.namelist():
                if name.endswith(".csv"):
                    r = zip_ref.extract(name, path=zip_path.parent)
                    extracted_path = zip_path.with_suffix(".csv")
                    # ƒë·ªïi t√™n ƒë·ªÉ kh·ªèi tr√πng
                    Path(r).rename(extracted_path)
                    log.info(f"ƒê√£ gi·∫£i n√©n file: {extracted_path}")
                    (inserted, replaced, total) = await self.import_file_trades_csv(symbol, market_type, extracted_path)
                    log.info(f"ƒê√£ import file: {extracted_path}, inserted: {Fore.GREEN}{inserted}{Style.RESET_ALL}, replaced: {Fore.YELLOW}{replaced}{Style.RESET_ALL}, total: {Fore.CYAN}{total}{Style.RESET_ALL}")

    